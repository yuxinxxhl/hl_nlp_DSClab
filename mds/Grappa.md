## GRAPPA

### 摘要

提出一种有效的预训练任务，与语义解析器结合时在四个流行的任务上获得了SOTA

### 介绍

![image-20211011151140673](images/Grappa/image-20211011151140673.png) 

 

text-to-sql例子是现有的，从现有的例子中抽象出一个模板。

使用现有的模板随机生成question-SQL例子，使用qS例子和它相应的表去训练模型

### 方法论

大量研究表示上下文无关语法分割有效

#### 同步上下文无关语法的数据合成(SCFG)

预训练任务--让BERT识别可替换的部分并学习底层逻辑模板，这样做的动机是鼓励模型识别基于逻辑性形式组成的表模式组件，这一点对大多数任务至关重要

从SPIDER中使用一些例子归纳出SCFG，使用SCFG和采样的表格合成TEXT2SQL例子,整个框架会从TEXT2SQL合成（映射 ）出多个QUESTION-SQL例子

通过将产生式规则中的非终结符换为终结符来得到例子，产生式规则β是从一些 (x, y) ∈ D 中替换终结符为非终结符得到(x:utterance y:SQL query)

question2sql例子，ROOT → <α, β>，α：在生成β时随机选择的相应自然语言问题，**通过手工替换实体/短语为相应的非终结符类型**。β：产生式规则。

Grappa使用的是RoBERT初始化

MLM loss / SSP loss 训练过程不是很理解。

